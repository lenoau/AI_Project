{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from spektral.layers import GCNConv\n",
    "\n",
    "file_path = \"./Dummy_01.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 로드\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 시간 데이터 처리\n",
    "def process_time_data(df):\n",
    "    # event_time을 datetime 형식으로 변환\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "    \n",
    "    # epc_code별로 시간 차이 계산\n",
    "    df['time_diff'] = df.groupby('epc_code')['event_time'].diff().dt.total_seconds().fillna(0)\n",
    "    \n",
    "    # 시간의 순환적 특성 인코딩\n",
    "    df['hour'] = df['event_time'].dt.hour\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Label Encoding\n",
    "def encode_categorical_data(df):\n",
    "    # hub_type, event_type 라벨 인코딩\n",
    "    le_hub = LabelEncoder()\n",
    "    le_event = LabelEncoder()\n",
    "    df['hub_type_encoded'] = le_hub.fit_transform(df['hub_type'])\n",
    "    df['event_type_encoded'] = le_event.fit_transform(df['event_type'])\n",
    "\n",
    "    # hub_event_combined 생성 후 인코딩\n",
    "    df['hub_event_combined'] = df['hub_type'] + '_' + df['event_type']\n",
    "    le_combined = LabelEncoder()\n",
    "    df['hub_event_encoded'] = le_combined.fit_transform(df['hub_event_combined'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 시계열 데이터 구성\n",
    "def create_sequences(df, seq_length):\n",
    "    sequences = []\n",
    "    for _, group in df.groupby('epc_code'):\n",
    "        group_data = group[['time_diff', 'hub_type_encoded', 'event_type_encoded', 'hub_event_encoded']].values\n",
    "        for i in range(len(group_data) - seq_length + 1):\n",
    "            sequences.append(group_data[i:i + seq_length])\n",
    "    return np.array(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-1. 그래프 데이터 생성\n",
    "def create_graph_data(df, seq_length):\n",
    "    nodes = []\n",
    "    adjacency_matrices = []\n",
    "    for _, group in df.groupby('epc_code'):\n",
    "        group_data = group[['time_diff', 'hub_type_encoded', 'event_type_encoded', 'hub_event_encoded']].values\n",
    "        for i in range(len(group_data) - seq_length + 1):\n",
    "            sequence = group_data[i:i + seq_length]\n",
    "            adjacency_matrix = np.ones((seq_length, seq_length))  # Fully connected graph\n",
    "            nodes.append(sequence)\n",
    "            adjacency_matrices.append(adjacency_matrix)\n",
    "    return np.array(nodes), np.array(adjacency_matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 데이터 정규화\n",
    "def normalize_data(sequences):\n",
    "    scaler = StandardScaler()\n",
    "    for feature_idx in range(sequences.shape[2]):\n",
    "        sequences[:, :, feature_idx] = scaler.fit_transform(sequences[:, :, feature_idx])\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 데이터 분할\n",
    "def split_data(sequences, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    X_train, X_temp = train_test_split(sequences, test_size=val_ratio + test_ratio, random_state=42)\n",
    "    X_val, X_test = train_test_split(X_temp, test_size=test_ratio / (val_ratio + test_ratio), random_state=42)\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. GNN + LSTM 모델 생성\n",
    "def build_gnn_lstm_model(seq_length, feature_dim, graph_input_dim):\n",
    "    graph_nodes = layers.Input(shape=(graph_input_dim, feature_dim), name=\"graph_nodes\")\n",
    "    adjacency_matrix = layers.Input(shape=(graph_input_dim, graph_input_dim), name=\"adjacency_matrix\")\n",
    "\n",
    "    x = GCNConv(64, activation=\"relu\")([graph_nodes, adjacency_matrix])\n",
    "    x = GCNConv(32, activation=\"relu\")([x, adjacency_matrix])\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    sequence_input = layers.Input(shape=(seq_length, feature_dim), name=\"sequence_input\")\n",
    "    y = layers.LSTM(128, activation=\"relu\", return_sequences=True)(sequence_input)\n",
    "    y = layers.LSTM(64, activation=\"relu\", return_sequences=False)(y)\n",
    "\n",
    "    combined = layers.concatenate([x, y])\n",
    "\n",
    "    z = layers.Dense(64, activation=\"relu\")(combined)\n",
    "    z = layers.Dense(seq_length * feature_dim, activation=\"sigmoid\")(z)\n",
    "    output = tf.reshape(z, (-1, seq_length, feature_dim))\n",
    "\n",
    "    model = models.Model(inputs=[graph_nodes, adjacency_matrix, sequence_input], outputs=output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 재구성 결과 시각화\n",
    "def visualize_reconstruction(X_test, reconstructed, n_samples=5):\n",
    "    fig, axes = plt.subplots(n_samples, 2, figsize=(10, 10))\n",
    "    for i in range(n_samples):\n",
    "        axes[i, 0].plot(np.mean(X_test[i], axis=1), label=\"Original\", alpha=0.7)\n",
    "        axes[i, 0].set_title(f\"Original Sequence {i+1}\")\n",
    "        axes[i, 0].legend()\n",
    "        axes[i, 1].plot(np.mean(reconstructed[i], axis=1), label=\"Reconstructed\", alpha=0.7, linestyle=\"--\")\n",
    "        axes[i, 1].set_title(f\"Reconstructed Sequence {i+1}\")\n",
    "        axes[i, 1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 재구성 오류 분포 시각화\n",
    "def plot_reconstruction_error(X_test, reconstructed):\n",
    "    errors = np.mean(np.square(X_test - reconstructed), axis=(1, 2))\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(errors, bins=50, alpha=0.75, color='blue', label='Reconstruction Error')\n",
    "    plt.axvline(np.mean(errors) + 2 * np.std(errors), color='red', linestyle='--', label='Threshold (Mean + 2*Std)')\n",
    "    plt.title(\"Reconstruction Error Distribution\")\n",
    "    plt.xlabel(\"Reconstruction Error\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. 이상치 탐지 및 시각화\n",
    "def detect_anomalies(errors, threshold):\n",
    "    total_samples = len(errors)\n",
    "    anomalies = errors > threshold\n",
    "    anomaly_count = np.sum(anomalies)\n",
    "    anomaly_ratio = anomaly_count / total_samples * 100\n",
    "    print(f\"Anomaly ratio: {anomaly_ratio:.2f}%\")\n",
    "    print(f\"Number of anomalies detected: {np.sum(anomalies)}\")\n",
    "    return anomalies\n",
    "\n",
    "def visualize_anomalies(X_test, reconstructed, anomalies, n_samples=5):\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    reconstructed = np.array(reconstructed)\n",
    "    \n",
    "    anomaly_indices = np.where(anomalies)[0]\n",
    "    \n",
    "    if len(anomaly_indices) == 0:\n",
    "        print(\"No anomalies detected.\")\n",
    "        return\n",
    "    \n",
    "    \n",
    "    n_samples = min(n_samples, len(anomaly_indices))\n",
    "    selected_indices = anomaly_indices[:n_samples]\n",
    "    \n",
    "    fig, axes = plt.subplots(n_samples, 2, figsize=(10, 10))\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        axes[i, 0].plot(np.mean(X_test[idx], axis=1), label=\"Original\", alpha=0.7)\n",
    "        axes[i, 0].set_title(f\"Anomalous Sequence {idx+1}: Original\")\n",
    "        axes[i, 0].legend()\n",
    "        \n",
    "        axes[i, 1].plot(np.mean(reconstructed[idx], axis=1), label=\"Reconstructed\", alpha=0.7, linestyle=\"--\")\n",
    "        axes[i, 1].set_title(f\"Anomalous Sequence {idx+1}: Reconstructed\")\n",
    "        axes[i, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. 파이프라인 실행\n",
    "def preprocess_pipeline(file_path, seq_length=10):\n",
    "    df = load_data(file_path)\n",
    "    df = process_time_data(df)\n",
    "    df = encode_categorical_data(df)\n",
    "    df = df.drop(columns=['product_serial', 'product_name'])\n",
    "    sequences = create_sequences(df, seq_length)\n",
    "    sequences = normalize_data(sequences)\n",
    "    X_train, X_val, X_test = split_data(sequences)\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling GCNConv.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'gcn_conv_2' (of type GCNConv). Either the `GCNConv.call()` method is incorrect, or you need to implement the `GCNConv.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nTried to convert 'y' to a tensor and failed. Error: None values not supported.\u001b[0m\n\nArguments received by GCNConv.call():\n  • args=(['<KerasTensor shape=(None, 10, 4), dtype=float32, sparse=False, name=graph_nodes>', '<KerasTensor shape=(None, 10, 10), dtype=float32, sparse=False, name=adjacency_matrix>'],)\n  • kwargs={'mask': ['None', 'None']}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m feature_dim \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m      7\u001b[0m graph_input_dim \u001b[38;5;241m=\u001b[39m seq_length\n\u001b[1;32m----> 8\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_gnn_lstm_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_input_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_input_dim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m history \u001b[38;5;241m=\u001b[39m autoencoder\u001b[38;5;241m.\u001b[39mfit(X_train, X_train, validation_data\u001b[38;5;241m=\u001b[39m(X_val, X_val), epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[60], line 6\u001b[0m, in \u001b[0;36mbuild_gnn_lstm_model\u001b[1;34m(seq_length, feature_dim, graph_input_dim)\u001b[0m\n\u001b[0;32m      3\u001b[0m graph_nodes \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(graph_input_dim, feature_dim), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_nodes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m adjacency_matrix \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(graph_input_dim, graph_input_dim), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjacency_matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mGCNConv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgraph_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjacency_matrix\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m GCNConv(\u001b[38;5;241m32\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)([x, adjacency_matrix])\n\u001b[0;32m      9\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mFlatten()(x)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\spektral\\layers\\convolutional\\conv.py:74\u001b[0m, in \u001b[0;36mcheck_dtypes_decorator.<locals>._inner_check_dtypes\u001b[1;34m(inputs, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(call)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_inner_check_dtypes\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     73\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m check_dtypes(inputs)\n\u001b[1;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\torch_book\\Lib\\site-packages\\spektral\\layers\\convolutional\\gcn_conv.py:106\u001b[0m, in \u001b[0;36mGCNConv.call\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m    104\u001b[0m     output \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mbias_add(output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 106\u001b[0m     \u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    107\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(output)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling GCNConv.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'gcn_conv_2' (of type GCNConv). Either the `GCNConv.call()` method is incorrect, or you need to implement the `GCNConv.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nTried to convert 'y' to a tensor and failed. Error: None values not supported.\u001b[0m\n\nArguments received by GCNConv.call():\n  • args=(['<KerasTensor shape=(None, 10, 4), dtype=float32, sparse=False, name=graph_nodes>', '<KerasTensor shape=(None, 10, 10), dtype=float32, sparse=False, name=adjacency_matrix>'],)\n  • kwargs={'mask': ['None', 'None']}"
     ]
    }
   ],
   "source": [
    "# 13. 실행\n",
    "seq_length = 10\n",
    "X_train, X_val, X_test = preprocess_pipeline(file_path, seq_length)\n",
    "\n",
    "# Autoencoder 생성 및 학습\n",
    "feature_dim = X_train.shape[2]\n",
    "graph_input_dim = seq_length\n",
    "autoencoder = build_gnn_lstm_model(seq_length=seq_length, feature_dim=feature_dim, graph_input_dim=graph_input_dim)\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "history = autoencoder.fit(X_train, X_train, validation_data=(X_val, X_val), epochs=5, batch_size=32, shuffle=True)\n",
    "\n",
    "# 학습 과정 시각화 추가\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n",
    "\n",
    "# 테스트 데이터 평가 및 결과 분석\n",
    "reconstructed = autoencoder.predict(X_test)\n",
    "visualize_reconstruction(X_test, reconstructed)\n",
    "reconstruction_errors = plot_reconstruction_error(X_test, reconstructed)\n",
    "\n",
    "# 이상치 탐지\n",
    "threshold = np.mean(reconstruction_errors) + 3 * np.std(reconstruction_errors)\n",
    "anomalies = detect_anomalies(reconstruction_errors, threshold)\n",
    "visualize_anomalies(X_test, reconstructed, anomalies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
