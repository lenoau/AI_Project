{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "file_path = \"./Dummy_02.csv\"\n",
    "\n",
    "# 1. 데이터 로드\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 시간 데이터 처리\n",
    "def process_time_data(df):\n",
    "    # event_time을 datetime 형식으로 변환\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'])\n",
    "    \n",
    "    # epc_code별로 시간 차이 계산\n",
    "    df['time_diff'] = df.groupby('epc_code')['event_time'].diff().dt.total_seconds().fillna(0)\n",
    "    \n",
    "    # 시간의 순환적 특성 인코딩\n",
    "    df['hour'] = df['event_time'].dt.hour\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Label Encoding\n",
    "def encode_categorical_data(df):\n",
    "    # hub_type, event_type 라벨 인코딩\n",
    "    le_hub = LabelEncoder()\n",
    "    le_event = LabelEncoder()\n",
    "    df['hub_type_encoded'] = le_hub.fit_transform(df['hub_type'])\n",
    "    df['event_type_encoded'] = le_event.fit_transform(df['event_type'])\n",
    "\n",
    "    # hub_event_combined 생성 후 인코딩\n",
    "    df['hub_event_combined'] = df['hub_type'] + '_' + df['event_type']\n",
    "    le_combined = LabelEncoder()\n",
    "    df['hub_event_encoded'] = le_combined.fit_transform(df['hub_event_combined'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 시계열 데이터 구성\n",
    "def create_sequences(df, seq_length):\n",
    "    sequences = []\n",
    "    for _, group in df.groupby('epc_code'):\n",
    "        group_data = group[['time_diff', 'hub_type_encoded', 'event_type_encoded', 'hub_event_encoded']].values\n",
    "        for i in range(len(group_data) - seq_length + 1):\n",
    "            sequences.append(group_data[i:i + seq_length])\n",
    "    return np.array(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 데이터 정규화\n",
    "def normalize_data(sequences):\n",
    "    scaler = StandardScaler()\n",
    "    for feature_idx in range(sequences.shape[2]):\n",
    "        sequences[:, :, feature_idx] = scaler.fit_transform(sequences[:, :, feature_idx])\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 데이터 분할\n",
    "def split_data(sequences, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    X_train, X_temp = train_test_split(sequences, test_size=val_ratio + test_ratio, random_state=42)\n",
    "    X_val, X_test = train_test_split(X_temp, test_size=test_ratio / (val_ratio + test_ratio), random_state=42)\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Autoencoder 모델 생성\n",
    "def build_autoencoder(seq_length, feature_dim):\n",
    "    input_seq = layers.Input(shape=(seq_length, feature_dim))\n",
    "    x = layers.LSTM(256, activation='relu', return_sequences=True)(input_seq)\n",
    "    x = layers.LSTM(128, activation='relu', return_sequences=True)(x)\n",
    "    x = layers.LSTM(64, activation='relu', return_sequences=False)(x)\n",
    "    latent = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.RepeatVector(seq_length)(latent)\n",
    "    x = layers.LSTM(64, activation='relu', return_sequences=True)(x)\n",
    "    x = layers.LSTM(128, activation='relu', return_sequences=True)(x)\n",
    "    x = layers.LSTM(256, activation='relu', return_sequences=True)(x)\n",
    "    output_seq = layers.TimeDistributed(layers.Dense(feature_dim))(x)\n",
    "    autoencoder = models.Model(input_seq, output_seq)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. 재구성 결과 시각화\n",
    "def visualize_reconstruction(X_test, reconstructed, n_samples=5):\n",
    "    fig, axes = plt.subplots(n_samples, 2, figsize=(10, 10))\n",
    "    for i in range(n_samples):\n",
    "        axes[i, 0].plot(np.mean(X_test[i], axis=1), label=\"Original\", alpha=0.7)\n",
    "        axes[i, 0].set_title(f\"Original Sequence {i+1}\")\n",
    "        axes[i, 0].legend()\n",
    "        axes[i, 1].plot(np.mean(reconstructed[i], axis=1), label=\"Reconstructed\", alpha=0.7, linestyle=\"--\")\n",
    "        axes[i, 1].set_title(f\"Reconstructed Sequence {i+1}\")\n",
    "        axes[i, 1].legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 재구성 오류 분포 시각화\n",
    "def plot_reconstruction_error(X_test, reconstructed):\n",
    "    errors = np.mean(np.abs(X_test - reconstructed), axis=(1, 2)) #mse -> abs로 교체 후 성능 개선\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.hist(errors, bins=50, alpha=0.75, color='blue', label='Reconstruction Error')\n",
    "    plt.axvline(np.mean(errors) + 2 * np.std(errors), color='red', linestyle='--', label='Threshold (Mean + 2*Std)')\n",
    "    plt.title(\"Reconstruction Error Distribution\")\n",
    "    plt.xlabel(\"Reconstruction Error\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. 이상치 탐지 및 시각화\n",
    "def detect_anomalies(errors, threshold):\n",
    "    total_samples = len(errors)\n",
    "    anomalies = errors > threshold\n",
    "    anomaly_count = np.sum(anomalies)\n",
    "    anomaly_ratio = anomaly_count / total_samples * 100\n",
    "    print(f\"Anomaly ratio: {anomaly_ratio:.2f}%\")\n",
    "    print(f\"Number of anomalies detected: {np.sum(anomalies)}\")\n",
    "    return anomalies\n",
    "\n",
    "def visualize_anomalies(X_test, reconstructed, anomalies, n_samples=5):\n",
    "    # Ensure data is numpy arrays\n",
    "    X_test = np.array(X_test)\n",
    "    reconstructed = np.array(reconstructed)\n",
    "    \n",
    "    anomaly_indices = np.where(anomalies)[0]\n",
    "    \n",
    "    if len(anomaly_indices) == 0:\n",
    "        print(\"No anomalies detected.\")\n",
    "        return\n",
    "    \n",
    "    # Ensure anomaly indices do not exceed available samples\n",
    "    n_samples = min(n_samples, len(anomaly_indices))\n",
    "    selected_indices = anomaly_indices[:n_samples]\n",
    "    \n",
    "    fig, axes = plt.subplots(n_samples, 2, figsize=(10, 10))\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        axes[i, 0].plot(np.mean(X_test[idx], axis=1), label=\"Original\", alpha=0.7)\n",
    "        axes[i, 0].set_title(f\"Anomalous Sequence {idx+1}: Original\")\n",
    "        axes[i, 0].legend()\n",
    "        \n",
    "        axes[i, 1].plot(np.mean(reconstructed[idx], axis=1), label=\"Reconstructed\", alpha=0.7, linestyle=\"--\")\n",
    "        axes[i, 1].set_title(f\"Anomalous Sequence {idx+1}: Reconstructed\")\n",
    "        axes[i, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. 파이프라인 실행\n",
    "def preprocess_pipeline(file_path, seq_length=10):\n",
    "    df = load_data(file_path)\n",
    "    df = process_time_data(df)\n",
    "    df = encode_categorical_data(df)\n",
    "    df = df.drop(columns=['product_serial', 'product_name'])\n",
    "    sequences = create_sequences(df, seq_length)\n",
    "    sequences = normalize_data(sequences)\n",
    "    X_train, X_val, X_test = split_data(sequences)\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss는 계속 감소하지만 val_loss가 증가하기 시작하면 과적합 신호입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습 : cpu\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else 'cpu')\n",
    "print(f'다음 기기로 학습 : {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. 실행\n",
    "seq_length = 10\n",
    "X_train, X_val, X_test = preprocess_pipeline(file_path, seq_length)\n",
    "\n",
    "# Autoencoder 생성 및 학습\n",
    "feature_dim = X_train.shape[2]\n",
    "autoencoder = build_autoencoder(seq_length, feature_dim)\n",
    "autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "history = autoencoder.fit(X_train, X_train, validation_data=(X_val, X_val), epochs=20, batch_size=32, shuffle=True)\n",
    "\n",
    "# 학습 과정 시각화 추가\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)\n",
    "\n",
    "# 테스트 데이터 평가 및 결과 분석\n",
    "reconstructed = autoencoder.predict(X_test)\n",
    "visualize_reconstruction(X_test, reconstructed)\n",
    "reconstruction_errors = plot_reconstruction_error(X_test, reconstructed)\n",
    "\n",
    "# 이상치 탐지\n",
    "threshold = np.mean(reconstruction_errors) + 2 * np.std(reconstruction_errors)\n",
    "anomalies = detect_anomalies(reconstruction_errors, threshold)\n",
    "visualize_anomalies(X_test, reconstructed, anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "#13. 모델 저장\n",
    "autoencoder.compile(optimizer='adam', loss=tf.keras.losses.MeanSquaredError(), metrics=['accuracy'])\n",
    "autoencoder.save('autoencoder_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
