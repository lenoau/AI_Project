{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed, Concatenate, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from spektral.layers import GCNConv\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다음 기기로 학습 : cuda\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else 'cpu')\n",
    "print(f'다음 기기로 학습 : {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 로드\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_time_features(df):\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'], errors='coerce')\n",
    "    df.dropna(subset=['event_time'], inplace=True)\n",
    "    df = df.sort_values(['epc_code', 'event_time'])\n",
    "    \n",
    "    # 시간 차 계산 (초 단위) 및 로그 변환 (0 포함 고려)\n",
    "    df['time_diff'] = df.groupby('epc_code')['event_time'].diff().dt.total_seconds().fillna(0)\n",
    "    df['log_time_diff'] = np.log1p(df['time_diff'])\n",
    "    \n",
    "    # 시간 관련 파생 피처\n",
    "    df['hour'] = df['event_time'].dt.hour\n",
    "    df['day_of_week'] = df['event_time'].dt.dayofweek  # 0: 월요일 ~ 6: 일요일\n",
    "    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    \n",
    "    # epc_code 그룹 내에서 이벤트 순서 (1부터 시작)\n",
    "    df['event_order'] = df.groupby('epc_code').cumcount() + 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_time_features(df):\n",
    "    df['event_time'] = pd.to_datetime(df['event_time'], errors='coerce')\n",
    "    df.dropna(subset=['event_time'], inplace=True)\n",
    "    df = df.sort_values(['epc_code', 'event_time'])\n",
    "    \n",
    "    # 시간 차 계산 (초 단위) 및 로그 변환 (0 포함 고려)\n",
    "    df['time_diff'] = df.groupby('epc_code')['event_time'].diff().dt.total_seconds().fillna(0)\n",
    "    df['log_time_diff'] = np.log1p(df['time_diff'])\n",
    "    \n",
    "    # 시간 관련 파생 피처\n",
    "    df['hour'] = df['event_time'].dt.hour\n",
    "    df['day_of_week'] = df['event_time'].dt.dayofweek  # 0: 월요일 ~ 6: 일요일\n",
    "    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    \n",
    "    # epc_code 그룹 내에서 이벤트 순서 (1부터 시작)\n",
    "    df['event_order'] = df.groupby('epc_code').cumcount() + 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_categories(df):\n",
    "    le_hub = LabelEncoder()\n",
    "    le_event = LabelEncoder()\n",
    "    df['hub_type_enc'] = le_hub.fit_transform(df['hub_type'])\n",
    "    df['event_type_enc'] = le_event.fit_transform(df['event_type'])\n",
    "    \n",
    "    # 결합 피처 (필요 시 사용)\n",
    "    df['hub_event_combined'] = df['hub_type'] + '_' + df['event_type']\n",
    "    le_combined = LabelEncoder()\n",
    "    df['hub_event_enc'] = le_combined.fit_transform(df['hub_event_combined'])\n",
    "    \n",
    "    return df, le_hub, le_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences(df, seq_length=10):\n",
    "    feature_cols = [\n",
    "        'log_time_diff', 'hour', 'day_of_week', 'is_weekend',\n",
    "        'event_type_enc', 'hub_event_enc', 'event_order'\n",
    "    ]\n",
    "    hub_index_col = 'hub_type_enc'\n",
    "    \n",
    "    sequence_features = []\n",
    "    hub_indices = []\n",
    "    \n",
    "    for epc, group in df.groupby('epc_code'):\n",
    "        group = group.sort_values('event_time')\n",
    "        data_features = group[feature_cols].values\n",
    "        data_hub_indices = group[hub_index_col].values\n",
    "        if len(data_features) < seq_length:\n",
    "            continue\n",
    "        # 슬라이딩 윈도우 방식으로 시퀀스 생성\n",
    "        for i in range(len(data_features) - seq_length + 1):\n",
    "            sequence_features.append(data_features[i:i+seq_length])\n",
    "            hub_indices.append(data_hub_indices[i:i+seq_length])\n",
    "            \n",
    "    sequence_features = np.array(sequence_features)\n",
    "    hub_indices = np.array(hub_indices)\n",
    "    return sequence_features, hub_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sequences(sequences):\n",
    "    num_samples, seq_length, num_features = sequences.shape\n",
    "    scaler = StandardScaler()\n",
    "    sequences_flat = sequences.reshape(-1, num_features)\n",
    "    sequences_scaled_flat = scaler.fit_transform(sequences_flat)\n",
    "    sequences_scaled = sequences_scaled_flat.reshape(num_samples, seq_length, num_features)\n",
    "    return sequences_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hub_graph(df):\n",
    "    edges = []\n",
    "    for epc, group in df.groupby('epc_code'):\n",
    "        group = group.sort_values('event_time')\n",
    "        hubs = group['hub_type'].values\n",
    "        for i in range(len(hubs) - 1):\n",
    "            edges.append((hubs[i], hubs[i+1]))\n",
    "            \n",
    "    G = nx.DiGraph()\n",
    "    for u, v in edges:\n",
    "        if G.has_edge(u, v):\n",
    "            G[u][v]['weight'] += 1\n",
    "        else:\n",
    "            G.add_edge(u, v, weight=1)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pipeline(file_path, seq_length=10):\n",
    "    df = load_data(file_path)\n",
    "    df = process_time_features(df)\n",
    "    df, le_hub, le_event = encode_categories(df)\n",
    "    df = df.drop(columns=['product_serial', 'product_name'], errors='ignore')\n",
    "    \n",
    "    sequence_features, hub_indices = build_sequences(df, seq_length)\n",
    "    sequence_features_scaled, scaler = normalize_sequences(sequence_features)\n",
    "    hub_graph = build_hub_graph(df)\n",
    "    \n",
    "    return sequence_features_scaled, hub_indices, hub_graph, scaler, le_hub, le_event\n",
    "\n",
    "# 파일 경로와 시퀀스 길이 설정\n",
    "file_path = \"Dummy_02.csv\" \n",
    "seq_length = 10            # 한 시퀀스에 포함할 이벤트 개수\n",
    "\n",
    "# 전처리 파이프라인 실행\n",
    "sequence_features_scaled, hub_indices, hub_graph, scaler, le_hub, le_event = preprocess_pipeline(file_path, seq_length)\n",
    "\n",
    "print(\"전처리된 시퀀스 데이터 shape:\", sequence_features_scaled.shape)  \n",
    "print(\"전처리된 hub_indices shape:\", hub_indices.shape)\n",
    "\n",
    "# GNN 입력용 그래프 데이터 준비\n",
    "num_nodes = 6\n",
    "print(\"고유 hub_type 수 (num_nodes):\", num_nodes)\n",
    "\n",
    "# 그래프의 노드 피처: 여기서는 단위행렬(One-hot) 사용 (shape: (num_nodes, num_nodes))\n",
    "graph_nodes_features = np.eye(num_nodes, dtype=np.float32)\n",
    "# 인접행렬: hub_graph를 이용하여 (shape: (num_nodes, num_nodes))\n",
    "# 노드 순서를 정렬하여 일관되게 처리합니다.\n",
    "A = np.eye(num_nodes, dtype=np.float32)\n",
    "\n",
    "graph_nodes_features = np.expand_dims(graph_nodes_features, axis=0)\n",
    "A = np.expand_dims(A, axis=0)\n",
    "\n",
    "# # 3. GNN + LSTM 모델 구성 (Spektral 활용)\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "sequence_feature_dim = sequence_features_scaled.shape[-1]  # 여기서는 7 (log_time_diff, hour, day_of_week, is_weekend, event_type_enc, hub_event_enc, event_order)\n",
    "   # GNN으로 얻을 hub 임베딩 차원\n",
    "\n",
    "# 1. **그래프(GNN) 분기**  \n",
    "# 그래프 입력: \n",
    "# - graph_nodes_input: 각 노드의 피처 (여기서는 one-hot 벡터, shape: (num_nodes,))\n",
    "# - A_input: 인접행렬 (shape: (num_nodes,))\n",
    "graph_nodes_input = Input(shape=(num_nodes, num_nodes), batch_size=1, name='graph_nodes_input')\n",
    "A_input = Input(shape=(num_nodes, num_nodes), batch_size=1, name='A_input')\n",
    "\n",
    "\n",
    "# GCNConv 레이어를 통해 각 노드의 임베딩 생성  \n",
    "embedding_dim = 16\n",
    "gnn_out = GCNConv(embedding_dim, activation='relu', name='gcn_conv')([graph_nodes_input, A_input])\n",
    "print(gnn_out)\n",
    "# gnn_out의 shape: (num_nodes, embedding_dim)\n",
    "\n",
    "# 2. **시퀀스(LSTM) 분기**  \n",
    "# 시퀀스 데이터 입력 (정규화된 나머지 피처들)\n",
    "sequence_input = Input(shape=(seq_length, sequence_feature_dim), name='sequence_input')\n",
    "# 시퀀스 내 각 이벤트에 해당하는 hub_type 인덱스 (정수, shape: (seq_length,))\n",
    "hub_index_input = Input(shape=(seq_length,), dtype='int32', name='hub_index_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda 레이어를 사용하여, gnn_out(노드 임베딩 행렬)에서 hub_index_input에 해당하는 임베딩을 lookup\n",
    "def lookup_embeddings(args):\n",
    "    hub_embeddings, hub_indices = args\n",
    "    return tf.gather(hub_embeddings, hub_indices)\n",
    "\n",
    "hub_embeddings_seq = Lambda(lookup_embeddings, name='hub_lookup')([gnn_out, hub_index_input])\n",
    "# hub_embeddings_seq의 shape: (batch_size, seq_length, embedding_dim)\n",
    "\n",
    "# 시퀀스의 다른 피처와 hub 임베딩을 concatenate하여 최종 입력 구성\n",
    "augmented_sequence = Concatenate(axis=-1, name='concat_features')([sequence_input, hub_embeddings_seq])\n",
    "# 최종 입력 shape: (batch_size, seq_length, sequence_feature_dim + embedding_dim)\n",
    "\n",
    "# LSTM 기반 오토인코더 구성 (인코더)\n",
    "encoder = LSTM(128, return_sequences=True, name='encoder_lstm1')(augmented_sequence)\n",
    "encoder = LSTM(64, return_sequences=False, name='encoder_lstm2')(encoder)\n",
    "latent = Dense(32, activation='relu', name='latent_dense')(encoder)\n",
    "\n",
    "# 디코더\n",
    "decoder_input = RepeatVector(seq_length, name='repeat_vector')(latent)\n",
    "decoder = LSTM(64, return_sequences=True, name='decoder_lstm1')(decoder_input)\n",
    "decoder = LSTM(128, return_sequences=True, name='decoder_lstm2')(decoder)\n",
    "\n",
    "# 재구성 출력 차원은 augmented_sequence와 동일\n",
    "decoder_output = TimeDistributed(Dense(sequence_feature_dim + embedding_dim), name='time_distributed')(decoder)\n",
    "\n",
    "# 모델 정의: 총 4개의 입력을 받음\n",
    "model = Model(\n",
    "    inputs=[sequence_input, hub_index_input, graph_nodes_input, A_input],\n",
    "    outputs=decoder_output\n",
    ")\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "y_train = sequence_features_scaled  # 정상 데이터라 가정\n",
    "\n",
    "# 모델 학습 (배치 사이즈와 epochs는 필요에 맞게 조정)\n",
    "history = model.fit(\n",
    "    x=[sequence_features_scaled, hub_indices, graph_nodes_features, A],\n",
    "    y=y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# 학습 결과 시각화 (Loss)\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss는 계속 감소하지만 val_loss가 증가하기 시작하면 과적합 신호입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
